{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"saJtPFsmH8Wx"},"source":["# Extracting tweets using the twitter API\n","\n","* Step 1: Get `Blodgett_50_IDs.csv` file from the Datasets folder. \n","* Step 2: Get access to twitter developer account and get your `twitter-api-secret.txt`.\n"]},{"cell_type":"markdown","metadata":{"id":"jGGm4n9WJRji"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GP1AsG73lRe1"},"outputs":[],"source":["import tweepy\n","import pandas as pd"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OBunhMK1JW4d"},"source":["## Reading Blodgett 50k IDs and the twitter secret key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ETirh39mJ3Q"},"outputs":[],"source":["df = pd.read_csv(\"Blodgett_50_IDS.csv\")\n","\n","#make sure to have twitter elevated access to extracted the tweets.\n","with open(\"twitter-api-secret.txt\") as api_file:\n","  api_secret = api_file.read().splitlines()"]},{"cell_type":"markdown","metadata":{"id":"QuQnl-hFJhBr"},"source":["## Authentication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3i7PTLl9mZcD"},"outputs":[],"source":["auth = tweepy.AppAuthHandler(api_secret[0], api_secret[1])\n","\n","api = tweepy.API(auth, wait_on_rate_limit=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4OLTb3FUJlZC"},"source":["## Dictionary where all the Tweet IDs and Tweet text will be stored"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waFbQiiWmtR-"},"outputs":[],"source":["data_dict = {\"Tweet_id\":[],\"Tweet_text\":[]}"]},{"cell_type":"markdown","metadata":{"id":"XRlS0DvGJ5BO"},"source":["## API call to fetch the tweets and store the same in the dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVYulo1xmrVl"},"outputs":[],"source":["for tweet_id in df[\"Tweet_id\"]:\n","  try:\n","    tweet = api.get_status(tweet_id)._json[\"text\"]\n","    data_dict[\"Tweet_id\"].append(tweet_id)\n","    data_dict[\"Tweet_text\"].append(tweet)\n","  except: # The exception handling is done to avoid the code from crashing when a certain tweet was not found as it was deleted or removed.\n","    pass\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gJCEREpLKBEj"},"source":["## Creating data frame from the dictionary and then generating a CSV file which will be used further."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSfEeEfbnsn4"},"outputs":[],"source":["tweet_df = pd.DataFrame(data_dict)\n","tweet_df.to_csv(\"Blodgett_50k.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiVo6AWZn4NL"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
